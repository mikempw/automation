# ═══════════════════════════════════════════════════════════
# F5 Insight Skills — Environment Configuration
# ═══════════════════════════════════════════════════════════
# Copy this file to .env and fill in your values.
#   cp .env.sample .env
#
# Only configure ONE LLM provider section (uncomment the
# one you want). Anthropic is the default.
# ═══════════════════════════════════════════════════════════

# ─── LLM Provider ────────────────────────────────────────
# Options: anthropic | openai | local
LLM_PROVIDER=anthropic

# ─── Anthropic (Claude) ─────────────────────────────────
# Get your key at https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-api03-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# ─── OpenAI (uncomment to use instead of Anthropic) ─────
# OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# OPENAI_MODEL=gpt-4o
# OPENAI_BASE_URL=https://api.openai.com/v1

# ─── Local LLM (Ollama, vLLM, llama.cpp, etc.) ──────────
# Any OpenAI-compatible /v1/chat/completions endpoint.
# LOCAL_LLM_BASE_URL=http://host.docker.internal:11434/v1
# LOCAL_LLM_MODEL=llama3:70b
# LOCAL_LLM_API_KEY=not-needed
